{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style='color:green; font-size:30px'>Generowanie obrazów w stylu Claude'a Monet za pomocą DCGAN</p>\n",
    "## Aleksandra Buchowicz, Filip Pazio, Tomasz Markowicz\n",
    "### <p style='color:green'>Wydział Matematyki i Nauk Informacyjnych, Politechnika Warszawska</p> <br> Warsztaty z Technik Uczenia Maszynowego</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style='color:green; font-size:30px'>Wstęp</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* GAN - metoda opracowana przez Iana Goodfellow zestawiająca dwie sieci neuronowe - generatora (artystę) i dyskryminatora (krytyka)\n",
    "* Generator tworzy własne obrazy, dyskryminator ocenia, czy są ze zbioru danych, czy spoza niego. Pierwszy dąży do 'oszukania' drugiego, zaś drugi stara się być coraz bardziej szczegółowy.\n",
    "* DCGAN (Deep Convolutional Generative Adversarial Networks) jest pewną klasą konwolucyjnych sieci neuronowych (CNN), które potrafią stopniowo filtrować różne części danych uczących i wyostrzać ważne cechy w procesie dyskryminacji wykorzystanym do rozpoznawania lub klasyfikacji wzorców."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-UYQLOSL5Hj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style='color:green; font-size:30px'>Importowanie modułów</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y18SPxxBK2wb",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv1zANW6i68t",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <p style='color:green; font-size:30px'>Dane - wczytanie i konwersja obrazów do wektorów</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Uk3qfXI_PLYm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir):\n",
    "        path_list = os.listdir(img_dir) #images names\n",
    "        abspath = os.path.abspath(img_dir) #absolute path of images\n",
    "\n",
    "        self.img_list = [os.path.join(abspath, path) for path in path_list] #full path\n",
    "\n",
    "        #set of transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(64),\n",
    "            transforms.CenterCrop(64),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]), #normalize [-1, 1]\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_list[index]\n",
    "        img = Image.open(path).convert('RGB') #image size (256, 256)\n",
    "        return self.transform(img)\n",
    "        #return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xKymfJKnPVO3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "db = Dataset('monet_jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hv2nqmgmo0kH",
    "outputId": "ff7e0f7a-671d-4612-ce25-8d2611f17f0d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYNjKfffi90s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TNVC5h2nL4GO",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, noise_size, img_dim=64): #img_dim=64\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=noise_size, out_channels=img_dim * 8, kernel_size=4, stride=1, padding=0, bias=False), \n",
    "            nn.BatchNorm2d(img_dim * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=img_dim * 8, out_channels=img_dim * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(img_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "           \n",
    "            nn.ConvTranspose2d(in_channels=img_dim * 4, out_channels=img_dim * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(img_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "           \n",
    "            nn.ConvTranspose2d(in_channels=img_dim * 2, out_channels=img_dim, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(img_dim),\n",
    "            nn.ReLU(True),\n",
    "          \n",
    "            nn.ConvTranspose2d(in_channels=img_dim, out_channels=3, kernel_size=4, stride=2, padding=1, bias=False), #out_channels RGB\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), x.size(1), 1, 1) \n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GTpNM7qWiCGJ",
    "outputId": "80eecec1-5449-4b17-a216-6c7d70675097",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generator(100, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6K683C0eiPW1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "        z = np.random.uniform(-1, 1, size=size)\n",
    "        return torch.from_numpy(z).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLAiztKuiQx2",
    "outputId": "ffc80777-8098-4e0a-8d4d-3e9eb8dfcd80",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9548, -0.1203,  0.8817,  0.0111, -0.4274, -0.1027, -0.2647, -0.1747,\n",
       "        -0.4267,  0.6905, -0.3098,  0.4478, -0.4889, -0.3943, -0.8687, -0.6387,\n",
       "         0.4656, -0.2567,  0.7280,  0.0462, -0.1148,  0.2342,  0.7135, -0.5119,\n",
       "        -0.1624, -0.1584,  0.9888,  0.7679,  0.5988,  0.1187, -0.4560,  0.8928,\n",
       "         0.7669, -0.3044, -0.8040, -0.8643, -0.2444,  0.5506, -0.0185,  0.7331,\n",
       "         0.8735,  0.4659, -0.1745,  0.4278, -0.4738,  0.7248, -0.4953,  0.8143,\n",
       "         0.4053, -0.9838,  0.9182, -0.3319, -0.6388, -0.4032,  0.1812,  0.4950,\n",
       "        -0.3103, -0.9802,  0.7568, -0.0794, -0.2338, -0.6118,  0.4085, -0.3002,\n",
       "        -0.5407,  0.2545, -0.4874, -0.2398, -0.6231,  0.9339, -0.5545,  0.7395,\n",
       "         0.4936,  0.5044,  0.4663, -0.2130, -0.0225,  0.4537,  0.8109, -0.6881,\n",
       "         0.7284, -0.5717, -0.1670,  0.9628,  0.6495,  0.6959, -0.9708,  0.9342,\n",
       "        -0.0550, -0.9821,  0.3412,  0.3833,  0.6039, -0.3914, -0.8125,  0.6423,\n",
       "        -0.4476, -0.6163, -0.2627,  0.7381])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-Ve1FG3i2Xs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Dyskryminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCyY6t_Ti2K_",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VNTrTGbXipXI",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, img_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, img_dim, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "   \n",
    "            nn.Conv2d(img_dim, img_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(img_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(img_dim * 2, img_dim * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(img_dim * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(img_dim * 4, img_dim * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(img_dim * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "          \n",
    "            nn.Conv2d(img_dim * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPSopB8QlBAq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##DCGAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yBrgxbl-lG2w",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "\n",
    "    def __init__(self, noise_size, img_dim):\n",
    "        self.noise_size = noise_size\n",
    "\n",
    "        self.D = Discriminator(img_dim)\n",
    "        self.G = Generator(noise_size, img_dim)\n",
    "\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    \n",
    "        self.D.to(self.device)\n",
    "        self.G.to(self.device)\n",
    "\n",
    "        self.D.apply(self.weights_init)\n",
    "        self.G.apply(self.weights_init)\n",
    "\n",
    "        \n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "            \n",
    "    def describe(self):\n",
    "        print('Discriminator')\n",
    "        print(self.D)\n",
    "\n",
    "        print('\\nGenerator')\n",
    "        print(self.G)\n",
    "\n",
    "    \n",
    "    def __calculate_loss(self, output, labels):\n",
    "        criterion = nn.BCELoss()\n",
    "        return criterion(output.squeeze(), labels)\n",
    "\n",
    "\n",
    "    def real_loss(self, D_out):\n",
    "        batch_size = D_out.size(0)\n",
    "        labels = torch.ones(batch_size).to(self.device)*0.8\n",
    "\n",
    "        return self.__calculate_loss(D_out, labels) \n",
    "\n",
    "\n",
    "    def fake_loss(self, D_out):\n",
    "        batch_size = D_out.size(0)\n",
    "        labels = torch.ones(batch_size).to(self.device)*0.1\n",
    " \n",
    "        return self.__calculate_loss(D_out, labels)\n",
    "\n",
    "\n",
    "    def noise(self, size):\n",
    "        z = np.random.uniform(-1, 1, size=size)\n",
    "        return torch.from_numpy(z).float().to(self.device)\n",
    "\n",
    "\n",
    "    def train_generator(self, g_optim, size):\n",
    "        g_optim.zero_grad()\n",
    "\n",
    "        z = self.noise(size)\n",
    "        fake_images = self.G(z)\n",
    "        \n",
    "        d_fake = self.D(fake_images)\n",
    "\n",
    "        g_loss = self.real_loss(d_fake)\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optim.step()\n",
    "\n",
    "        return g_loss.item()\n",
    "\n",
    "    \n",
    "    def train_discriminator(self, d_optim, real_images, size):\n",
    "        d_optim.zero_grad()\n",
    "\n",
    "        d_real = self.D(real_images.to(self.device)).view(-1)\n",
    "        d_real_loss = self.real_loss(d_real)\n",
    "\n",
    "        z = self.noise(size)\n",
    "        fake_images = self.G(z)\n",
    "      \n",
    "        d_fake = self.D(fake_images)\n",
    "        d_fake_loss = self.fake_loss(d_fake)\n",
    "\n",
    "        d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "        d_loss.backward()\n",
    "        d_optim.step()\n",
    "\n",
    "        return d_loss.item()\n",
    "\n",
    "\n",
    "    def train(self, num_epochs, d_optim, g_optim, data_loader, z_size, sample_size, print_every=500):\n",
    "        samples, losses = [], []\n",
    "\n",
    "        z = self.noise((sample_size, z_size))\n",
    "\n",
    "        self.D.train()\n",
    "        self.G.train()\n",
    "\n",
    "        print(f'Running on {self.device}')\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, real_images in enumerate(data_loader):                    \n",
    "                batch_size = real_images.size(0)\n",
    "\n",
    "                d_loss = self.train_discriminator(d_optim, real_images, (sample_size, z_size))\n",
    "                g_loss = self.train_generator(g_optim, (sample_size, z_size))\n",
    "\n",
    "                if i % print_every == 0:\n",
    "                    print('Epoch [{:5d}/{:5d}] | d_loss {:6.4f} | g_loss {:6.4f}'.format(\n",
    "                        epoch+1,\n",
    "                        num_epochs,\n",
    "                        d_loss,\n",
    "                        g_loss\n",
    "                    ))\n",
    "\n",
    "            losses.append( (d_loss, g_loss) )\n",
    "\n",
    "            self.G.eval()\n",
    "            samples.append( self.G(z) )\n",
    "            self.G.train()\n",
    "\n",
    "        with open('DCGAN_Sample_Output.pkl', 'wb') as f:\n",
    "            pkl.dump(samples, f)\n",
    "\n",
    "        return samples, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPAaGnnnlnNu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##TRENING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgOfKPjqlo_L",
    "outputId": "8eb70eb2-8490-46e0-f208-0f5b82ed4d1c",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n",
      "Epoch [    1/   50] | d_loss 1.7593 | g_loss 3.7251\n",
      "Epoch [    2/   50] | d_loss 1.0935 | g_loss 5.0222\n",
      "Epoch [    3/   50] | d_loss 1.4951 | g_loss 8.3511\n",
      "Epoch [    4/   50] | d_loss 1.9128 | g_loss 12.7523\n",
      "Epoch [    5/   50] | d_loss 1.1358 | g_loss 7.0221\n",
      "Epoch [    6/   50] | d_loss 1.6708 | g_loss 4.7288\n",
      "Epoch [    7/   50] | d_loss 1.7643 | g_loss 8.1294\n",
      "Epoch [    8/   50] | d_loss 0.8737 | g_loss 2.2486\n",
      "Epoch [    9/   50] | d_loss 0.9156 | g_loss 2.8011\n",
      "Epoch [   10/   50] | d_loss 0.8941 | g_loss 1.9960\n",
      "Epoch [   11/   50] | d_loss 1.1292 | g_loss 1.7881\n",
      "Epoch [   12/   50] | d_loss 0.9936 | g_loss 1.7927\n",
      "Epoch [   13/   50] | d_loss 1.0587 | g_loss 2.2001\n",
      "Epoch [   14/   50] | d_loss 1.1102 | g_loss 2.2017\n",
      "Epoch [   15/   50] | d_loss 1.0742 | g_loss 2.4338\n",
      "Epoch [   16/   50] | d_loss 1.0034 | g_loss 2.1054\n",
      "Epoch [   17/   50] | d_loss 1.0880 | g_loss 1.6877\n",
      "Epoch [   18/   50] | d_loss 1.1160 | g_loss 1.9800\n",
      "Epoch [   19/   50] | d_loss 0.8953 | g_loss 2.3929\n",
      "Epoch [   20/   50] | d_loss 0.8842 | g_loss 1.7948\n",
      "Epoch [   21/   50] | d_loss 0.9067 | g_loss 2.6847\n",
      "Epoch [   22/   50] | d_loss 0.9992 | g_loss 3.1318\n",
      "Epoch [   23/   50] | d_loss 0.9516 | g_loss 1.9298\n",
      "Epoch [   24/   50] | d_loss 0.8849 | g_loss 2.3012\n",
      "Epoch [   25/   50] | d_loss 0.8871 | g_loss 2.7148\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "monet_dataset = Dataset('monet_jpg') #data_dir\n",
    "data_loader = DataLoader(monet_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "noise_size = 128\n",
    "img_size = 64\n",
    "\n",
    "# Model\n",
    "dcgan_model = DCGAN(noise_size, img_size)\n",
    "\n",
    "# Optimizer\n",
    "lr = 0.0002\n",
    "beta1=0.5\n",
    "beta2=0.999\n",
    "\n",
    "d_optimizer = optim.Adam(dcgan_model.D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(dcgan_model.G.parameters(), lr, [beta1, beta2])\n",
    "\n",
    "# train\n",
    "EPOCHS = 50\n",
    "sample_size = 16 #nie wiem co to\n",
    "sample_result, losses_history = dcgan_model.train(EPOCHS, d_optimizer, g_optimizer, data_loader, noise_size, 16, print_every=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C2xw8FBHpX6U",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i, real_images in enumerate(data_loader):                    \n",
    "  print(real_images.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSeRHGbCnRfo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Wystawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "AuaKB_PknQy4",
    "outputId": "35c66248-443e-4fc7-f867-e6c1696f20d4",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(15,10), nrows=2, ncols=4, sharey=True, sharex=True)\n",
    "for ax, img in zip(axes.flatten(), sample_result[EPOCHS-1]):\n",
    "    _, w, h = img.size()\n",
    "     \n",
    "    img = img.detach().cpu().numpy()\n",
    "\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    \n",
    "    img = ((img +1)*255 / (2)).astype(np.uint8)\n",
    "\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    \n",
    "    im = ax.imshow(img.reshape((w,h,3)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmVIKOMkMcKn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AceVSMuMog0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv_8-jgTMb2R",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZXNElEZMUpZ",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "pz0JR5egMLHl",
    "outputId": "69df4379-a5cb-482b-c970-851e25963a66",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "EmVIKOMkMcKn"
   ],
   "name": "dcgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
